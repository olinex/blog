---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 特征归一化\(Feature Scaling\)

## 为什么要归一化

当我们在进行梯度下降时, 我们会发现在某些情况下, 我们需要循环很多次才能得到想要的结果. 原因是每一次梯度下降的循环后, 新的参数集Θ内总有很大一部分的参数并没有得到优化. 如果我们画出等值线图, 就会发现, 参数集的更新方向并不总是指向最低点, 而是存在一种"来回震荡"的现象.

![&#x6765;&#x81EA;Andrew Ng&#x6559;&#x6388;&#x7684;PPT](../../../.gitbook/assets/ahr0cdovl2ltzy5ibg9nlmnzzg4ubmv0lziwmtcwnzezmtq0mdi1mdqx.jpeg)

并且这类情况都有一个显著的特点: X内的某些特征值的取值范围远大于另外一些特征值. 从梯度下降的公式来看:

$$
\theta_n := \theta_n - \alpha\frac{\partial}{\partial\theta_n}J(\vec{\Theta})
$$

x值会对θ的参数变化有不同程度的影响, 我们假设目标函数h为二元一次函数, 则他们的梯度下降算法为:

$$
\theta_0 :=\theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h(\vec{X}^{(i)}) - y^{(i)})\\
\theta_1 := \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^m(h(\vec{X}^{(i)}) - y^{(i)})x_1^{(i)}\\
\theta_2 := \theta_2 - \alpha\frac{1}{m}\sum_{i=1}^m(h(\vec{X}^{(i)}) - y^{(i)})x_2^{(i)}
$$

由此可以看出, x可能的取值范围越大, 对应的θ变化也越大, 也越不容易指向最优的参数集Θ. 因此, 我们需要对特征值进行一定变换, 来减少特征值的取值范围对梯度下降的影响. 这种特征值处理方式叫做特征归一化.

## 如何归一化

特征归一化顾名思义, 就是让每个特征项可能的取值范围都尽量地落在一个很小的范围内, 这个范围以-3到+3之间为佳.

经过变换后, 样本的特征值也会发生变化, 我们将函数f定义为特征归一化的变换函数, 其输入为特征值x, 输出为变换后的特征值z:

$$
z_n = f_n(x_n)
$$

{% hint style="warning" %}
需要注意的是, 各项特征值xn, 他们有各自不同的变换函数fn. 我们不能简单地将每项特征都用同一个变换函数进行处理. 比较直观地说, 用同一个变换函数, 只能让损失函数的等高线图进行等比例缩放, 而不是变得更加"圆润".
{% endhint %}

需要特别注意的是, 当我们通过特征归一化获得参数集Θ后, 是不能直接将其代入目标函数h内的. 在此之间, 目标函数需要进行一次转换, 我们以n元一次函数举例:

$$
h(\vec{X}) = \theta_0 + 
\theta_1f_1(x_1) + 
\theta_2f_2(x_2) +
... +
 \theta_nf_n(x_n)
$$

而进行归一化的方式有很多种:

### 最大值归一

取每项特征中的最大值作为除数:

$$
z_n = f_n(x_n) = \frac{x_n}{max(x_n)}
$$



### 均值归一

取每项特征的平均值与取值范围的除数:

$$
z_n = f_n(x_n) = \frac{x_n - \mu_n}
{max(x_n) - min(x_n)}
$$

其中μn为每项特征的均值.

