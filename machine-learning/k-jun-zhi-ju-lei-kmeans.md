---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# K均值聚类\(K-Means\)

K-Means聚类算法是非常流行的一种无监督学习算法, 它原理简单且易于实现. 同时能够在后续持续增加训练集数据的情况下, 高效地更新模型. 这种聚类算法认为, 在以各个特征为轴的多维特征空间中, 每个样本都可以用空间中的点表示, 越相似的样本在空间中的距离越近, 反之越不相似的样本在空间中的距离越远. 因此, 当我们所有的样本仅有K种类型的时候, 每个样本应该都有且仅属于某一种类型, 属于同一种类型的样本应该在空间中彼此靠近.

## 设置K值

K-Means算法需要在进行训练时, 需要预先设置分类的个数, 即K值. 不同的K值对训练结果有严重的影响. 一般情况下, 需要我们结合问题的现实情况以及训练集的采集情况做一个较为合理的预估.

## 随机选取簇类中心

当我们人为设定好K值后, 我们就期望K-Means算法能够将训练集的数据分隔成多份, 我们称每一份数据为**簇\(Cluster\)**. 同时每个簇都会有一个中心, 称为**簇类中心\(Cluster centriod\)**. 

一开始的时候, 我们并不知道该如何分隔各个簇, 也不知道每个簇的中心在什么地方. 因此我们可以随机选取样本集中的K个样本, 作为簇类中心, 我们用**符号μ**来表示簇类中心点在特征空间中的向量, 用**符号c**来表示簇类中心点的序号.

$$
M_c = \{ \mu_i^{(c)}  | 1 \le i \le n\}
$$

$$
M_c = 
\left [
\begin{matrix}
\mu_1^{(c)}\\
\mu_2^{(c)}\\
...\\
\mu_n^{(c)}
\end{matrix}
\right ]\\
$$

## 簇类分配

为每个簇类随机分配簇类中心之后, 我们便可以为每个样本分配簇类. 通过与每个簇类中心进行特征空间距离的比较, 我们可以找到与样本最靠近的簇类中心, 并为其分配簇类:

$$
c := index(minimize \Vert X^{(i)} - M_c \Vert)
$$

其中c为样本i所归属的簇类序号.

## 调整簇类中心

当我们为每一个样本分配簇类后, 我们便可以重新设定簇类中心的位置了. 将属于同一个簇类的样本的各项特征值累加取平均, 变能够算得样本的中心点, 也即是该簇类新的中心.

$$
M^{(c)} 
:=
\frac{1}{m_c}
\sum_{i=1}^{m_c}X_i^{(c)}
$$

其中mc为簇类c中的样本个数. 得到新的簇类中心后, 不断重复簇类分配和调整簇类中心的步骤, 便可以不断优化簇类中心的位置. 最终, 当每个簇类中心的位置不再改变后, 我们便得到了簇类中心的局部最优解. 有此可以对未知的样本进行预测.

![&#x6765;&#x81EA;&#x4E8E;&#x7F51;&#x7EDC; https://blog.csdn.net/zhou191954/article/details/8267333](../.gitbook/assets/k-means.jpg)

## 局部最优解

对于K-Means算法而言, 虽然我们能够保证, 在不断进行簇类分配和调整簇类中心的动作下, 我们总能找到K个簇类中心, 他们会最终稳定到一个点不会再随着改变, 也就是K-Means算法一定能收敛到一个最优解. 但是这个最优解有可能只是局部的. 

尤其是对于那些仅为个位数的K值, 以及大量样本的情况, 非常有可能会掉入局部最优解.

