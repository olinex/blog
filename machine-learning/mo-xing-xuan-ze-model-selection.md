---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 模型选择\(Model Selection\)

当我们在进行机器学习, 企图寻找到一个合适的模型, 来进行预测/分类/聚类的时候, 模型的选择就是我们无法绕过的一个问题. 我们的目标是在我们人为设定的一个函数集合H内, 找到最为合适的函数h. 但我们在设定一个函数集合的时候, 容易出现一个逻辑矛盾: **我们既然不知道最为合适的函数是什么样子的, 那怎么能够确保我们设定的函数集合包含这个目标函数**.

当我们进行模型训练的时候, 我们总是会企图收集更多的样本, 希望以此找到目标函数. 但往往在做无用功. 因为目标函数根本不在函数集合内.

为了解决这个问题, 我们可以通过**交叉验证**的方式, 寻找到合适的模型.

## 交叉验证\(Cross Validation\)

我们将训练集D切分成三份:

* 训练集D\(train\) - 60%
* 交叉验证集D\(cv\) - 20%
* 测试集D\(test\) - 20%

他们分别占据总的训练集D的一部分, 我们假设有三个模型需要我们进行选择:

$$
H_1 = 
\{ 
h | h(X) = \theta_0 + \theta_1x
\}\\
H_2 = 
\{ 
h | h(X) = \theta_0 + \theta_1x + \theta_2x^2
\}\\
H_2 = 
\{ 
h | h(X) = \theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3
\}
$$

要在这三个模型当中选择一个合适的模型, 需要按照以下步骤进行挑选验证.

### 训练

通过训练集D\(train\), 分别训练三个模型, 获得他们的参数集Θ

$$
minimizeJ(H_1, D^{(train)}) \to \Theta_1\\
minimizeJ(H_2, D^{(train)}) \to \Theta_2\\
minimizeJ(H_3, D^{(train)}) \to \Theta_3
$$

### 交叉验证

使用我们训练出来的参数集Θ与交叉验证集D\(cross validation\), 计算出各个模型的损失函数值:

$$
J(\Theta_1, D^{(cv)}) \to j_1^{(cv)}\\
J(\Theta_2, D^{(cv)}) \to j_2^{(cv)}\\
J(\Theta_3, D^{(cv)}) \to j_3^{(cv)}
$$

根据最小的损失函数值, 我们能够找到对数据拟合做好的模型与参数集合Θ.

### 测试

通过交叉验证之后, 找到的模型与参数集合我们并不能确保训练集和验证集的数据存在相同的倾向. 因此需要再进行一次测试:

$$
J(\Theta_1, D^{(test)}) \to j_1^{(test)}\\
J(\Theta_2, D^{(test)}) \to j_2^{(test)}\\
J(\Theta_3, D^{(test)}) \to j_3^{(test)}
$$

若测试出来的损失函数值依然符合验证的分布, 我们可以大概率地确保我们选择了一个相对正确的模型.

## 过拟合与欠拟合

通过交叉验证, 我们能够有效地找到合适的模型, 这个过程事实上就是在避免过拟合与欠拟合的发生. 假设我们有多个模型, 其中有的模型对应的函数次数过低, 函数集合H内, 并没有包含我们的目标函数, 这会导致欠拟合, 通常在训练集D\(test\)和交叉验证集D\(cv\)都会有很高的损失函数值且相似:

$$
j^{(test)} \approx j^{(cv)}
$$

我们假设有多个模型, 其中有的模型对应的的函数次数过高, 这回导致过拟合, 通常在训练集D\(test\)有较低的损失函数值, 在交叉验证集D\(cv\)则有较高的损失函数值:

$$
j^{(test)} \ll j^{(cv)}
$$

![&#x56FE;&#x7247;&#x6765;&#x81EA;&#x4E8E;&#x7F51;&#x7EDC; https://zhuanlan.zhihu.com/p/25720278](../.gitbook/assets/v2-a576135bb0e2c6e009c744158ce246e1_1200x500.jpg)

## 偏斜类\(Skewed Classes\)

在逻辑回归问题中, 当我们在使用训练集对我们的模型进行训练的时候, 训练集的数据有可能也会对模型的结果产生不可忽视的影响. 我们假设我们要通过肿瘤的核磁共振成像图片判断患者是否罹患肿瘤. 通过1000张图片的训练, 我们最终得到了一个准确率有99%的模型.

但是这1000张图片存在严重的偏斜, 即其中只有10张图片的患者的确罹患了肿瘤. 这恰巧与模型的准确率一致. 也即是说, 我们即使设定了一个模型, 这个模型不管图片是什么都会判断没有肿瘤. 而这个模型也拥有99%的准确率.

这种数据本身存在极端偏斜的情况, 我们称之为偏斜类. 所以要判断一个模型是否足够优秀, 不能仅仅通过**准确率\(Accuracy\)**来判断, 还需要**查准率\(Precision\)**和**召回率\(Recall\)**.

对于多项分布的逻辑回归问题而言, 我们都会将其拆分成多个二项分布的逻辑回归问题, 因此对于同一个样本的判断结果而言, 都有四种可能:

* 真阳性 - 即真实结果为阳性, 判断结果也为阳性
* 真阴性 - 即真实结果为阴性, 判断结果也为阴性
* 假阳性 - 即真实结果是阴性, 判断结果为阳性
* 假阴性 - 即真实结果为阳性, 判断结果为阴性

因此我们可以定义出查准率和召回率:

### 查准率\(**Precision**\)

所谓的查准率, 即为:

$$
\frac
{预测为真阳性的个数}
{预测为阳性的个数}
=
\frac
{真阳性}
{真阳性+假阳性}
$$

对于高查准率的模型而言, 我们可以更加信赖其判断为阳性的结果.

### 召回率\(Recall\)

所谓召回率, 即为:

$$
\frac
{预测为真阳性的个数}
{真实结果为阳性的个数}
=
\frac
{真阳性}
{真阳性+假阴性}
$$

通过以上两个指标, 我们可以有效避免偏斜类的训练集误导我们对模型的判断.

查准率和召回率往往是互相矛盾的, 虽然我们的确可以找到查准率和召回率都很高的模型, 但现实当中这种情况并不多见. 同时他们还与我们的进行判断的阈值相关, 例如我们仅在目标函数输出的概率超过70%的时候判断结果为阳性, 则会有较高的查准率和较低的召回率. 同样的, 如果我们在目标函数输出的概率超过30%的时候判断结果为阳性, 则会有较高的召回率和较低的查准率.

## 选择标准

我们已经知道了, 模型的选择需要准确率, 查准率和召回率来共同判断, 但究竟应该通过什么标准来告诉我们自己, 什么模型更加具有优势呢?

我们需要使用**F1评分\(F1 score\)**来进行判断:

$$
F_1 
= 
\frac
{2 \times 查准率 \times 召回率}
{查准率 + 召回率}
$$

