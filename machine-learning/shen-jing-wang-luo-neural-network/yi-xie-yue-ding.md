---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 一些约定

## 层数

神经网络具有多个层次, 我们用字母L表示神经网络总的层数.

神经网络一般至少含有三个层次, 我们约定输入层的层数为1

## 序数

神经每一层都可能有多个单元, 我们约定它们的第一个单元为偏置单元, 其序数为1, 每一层的单元个数使用S来表示. 需要注意的是, 神经网络中, 每一层的单元个数都可以指定. 而输入层的单元个数与特征向量的维度有关, 输出层的单元个数与输出值y的分布有关:

$$
S_0 = n + 1
$$

当使用神经网络表征线性回归问题或二项分布的逻辑回归问题时:

$$
S_{L-1} = 1
$$

当使用神经网络表征多项分布的逻辑回归问题时:

$$
S_{L-1} = k
$$

其中k为多项分布的项数.

## 权重

神经网络中, 每个逻辑单元都有各自的权重向量W, 因此第j层的第i个单元的权重向量W表示为:

$$
\vec{W}_i^{(j)} = 
\left [
\begin{matrix}
w_{i0}^{(j)} \\
w_{i1}^{(j)}\\
...\\ 
w_{iS_{(j - 1)}}^{(j)}
\end{matrix}
\right ]
$$

其中, S\(j - 1\)为上一层的单元个数. 输入层并没有权重, 因此权重的j &gt; 0 . 同时因为每一层隐藏层的第一个单元为数字单元, 因此权重向量的i &gt; 0.

## 激励值

激励值即自变量向量X通过激励函数g计算后得到的数值, 和因变量y在意义上是等价的. 那为什么还要多余定义一个激励值呢? 这更多地是为了表述的方便, 因为因变量y的含义在很多范畴上都存在. 为了仅针对神经网络隐藏层中逻辑单元的激励函数g计算出的过程值, 我们定义了激励值, 并用a来表述, 并定义激励值向量A:

$$
\vec{A}^{(j)} = 
\left [
\begin{matrix}
a_0^{(j)}\\
a_1^{(j)}\\
...\\
a_{S_{(j - 1)}}^{(j)}
\end{matrix}
\right ]\\

\vec{A}^{(0)} = \vec{X}\\
$$

需要注意的是, 神经网络每一层单元的个数都不是固定的, 也因此每个逻辑单元权重向量W的元素个数, 事实上取决于上一层单元的个数. 我们假设激励值向量A内元素的个数为n, 并不是代表不同层次的逻辑单元的权重个数相等, 也不代表权重个数与特征向量X的维度相等. 仅有第一层隐藏层的逻辑单元的权重向量W的元素个数与特征向量X的维度相关.

对于第一层隐藏层而言:

$$
a_i^{(1)} = g(\vec{W}_{i}^{(1)} \cdot \vec{A}^{(0)})
$$

$$
a_1^{(1)} = g(\vec{W}_{1}^{(1)} \cdot \vec{X})\\
= g(
w_{10}^{(1)}x_0 + 
w_{11}^{(1)}x_1 + 
w_{12}^{(1)}x_2 + 
... + 
w_{1n}^{(1)}x_n
)\\
=g(\sum_{i=0}^nw_{1i}^{(1)}x_i)
$$

对于后续第j层的第i个逻辑单元而言:

$$
a_i^{(j)} = g(\vec{W}_{i}^{(j)} \cdot \vec{A}^{(j-1)})
$$

$$
a_i^{(j)} = g(\vec{W}_{i}^{(j)} \cdot \vec{A}^{(j-1)})\\
= g(
w_{i0}^{(j)}a_0^{(j-1)} + 
w_{i1}^{(j)}a_1^{(j-1)} +
w_{i2}^{(j)}a_2^{(j-1)} +
... +
w_{i{S_{(j-1)}}}^{(j)}a_{S_{(j-1)}}^{(j-1)}
)\\
=g(\sum_{i=0}^{S_{(j-1)}}w_{1i}^{(j)}a_i^{(j-1)})
$$

