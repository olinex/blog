---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 一些约定

## 网络层数

神经网络具有多个层次, 我们用字母L表示神经网络的网络层数.

神经网络一般至少含有三个层次, 我们约定输入层的网络层数为1

## 单元序数

神经网络每一层都可能有多个单元, 我们约定每一层除了偏置单元外的单元个数为单元序数, 使用字母S来表示. 需要注意的是, 神经网络中, 每一层的单元个数都可以指定. 而输入层的单元个数与特征向量的维度相等, 输出层的单元个数与输出值y的分布有关:

$$
S_1 = n
$$

当使用神经网络表征线性回归问题或二项分布的逻辑回归问题时:

$$
S_{L} = 1
$$

当使用神经网络表征多项分布的逻辑回归问题时:

$$
S_{L} = k
$$

其中k为多项分布的项数.

## 权重

神经网络中, 每个逻辑单元都有各自的权重向量W, 因此第j层的第i个单元的权重向量W表示为:

$$
\vec{W}_i^{(j)} = 
\left [
\begin{matrix}
w_{i0}^{(j)} \\
w_{i1}^{(j)}\\
...\\ 
w_{iS_{(j - 1)}}^{(j)}
\end{matrix}
\right ]
$$

其中, S\(j - 1\)为上一层的单元序数. 输入层并没有权重, 因此权重的j &gt; 1 .

## 激励值

激励值即自变量向量X通过激励函数g计算后得到的数值, 和因变量y在意义上是等价的. 那为什么还要多余定义一个激励值呢? 这更多地是为了表述的方便, 因为因变量y的含义在很多范畴上都存在. 为了仅针对神经网络隐藏层中逻辑单元的激励函数g计算出的过程值, 我们定义了激励值, 并用a来表述, 并定义激励值向量A:

$$
\vec{A}^{(j)} = 
\left [
\begin{matrix}
a_0^{(j)}\\
a_1^{(j)}\\
...\\
a_{S_{j}}^{(j)}
\end{matrix}
\right ]\\

\vec{A}^{(1)} = \vec{X}\\
$$

需要注意的是, 神经网络每一层单元的个数都不是固定的, 也因此每个逻辑单元权重向量W的元素个数, 事实上取决于上一层单元的个数.

对于第一层隐藏层而言:

$$
a_i^{(2)} = g(\vec{W}_{i}^{(2)} \cdot \vec{A}^{(1)})
$$

$$
a_1^{(2)} = g(\vec{W}_{1}^{(2)} \cdot \vec{X})\\
= g(
w_{10}^{(2)}x_0 + 
w_{11}^{(2)}x_1 + 
w_{12}^{(2)}x_2 + 
... + 
w_{1n}^{(2)}x_n
)\\
=g(\sum_{i=0}^nw_{1i}^{(2)}x_i)
$$

对于后续第j层的第i个逻辑单元而言:

$$
a_i^{(j)} = g(\vec{W}_{i}^{(j)} \cdot \vec{A}^{(j-1)})
$$

$$
a_i^{(j)} = g(\vec{W}_{i}^{(j)} \cdot \vec{A}^{(j-1)})\\
= g(
w_{i0}^{(j)}a_0^{(j-1)} + 
w_{i1}^{(j)}a_1^{(j-1)} +
w_{i2}^{(j)}a_2^{(j-1)} +
... +
w_{i{S_{(j-1)}}}^{(j)}a_{S_{(j-1)}}^{(j-1)}
)\\
=g(\sum_{i=0}^{S_{(j-1)}}w_{1i}^{(j)}a_i^{(j-1)})
$$



