---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 损失函数最小化

神经网络可以表征线性回归和逻辑回归, 其中主要的差别在于激励函数g的区别. 在这里, 我们使用最常见的sigmoid函数作为逻辑回归的激励函数, 并使用梯度下降进行损失函数最小化.

## 线性回归

当表征线性函数时, 损失函数为:

$$
J(\vec{W}) 
= \frac{1}{2m}
(\sum_{i=1}^m
(
h(\vec{X}^{(i)})
- y^{(i)}
)^2 + 
\lambda
\sum_{j=2}^L
\Vert \vec{W}^{(j)} \Vert ^2
)
$$

$$
\Vert \vec{W}^{(j)} \Vert ^2 = 
\sum_{i=1}^{S_j}
\sum_{j=0}^{S_{(j - 1)}}
w_{ij}^{(j)}
$$

## 逻辑回归

当使用神经网络表征逻辑回归时, 因为逻辑回归的不同分布, 需要将各个可能项的损失函数累加:

$$
J(\vec{W}) =
-\frac{1}{m}(
\sum_{i=1}^m
\sum_{k=1}^K
(
y_k^{(i)} \times log(h(\vec{X}^{(i)}))_k + 
(1 - y_k^{(i)}) \times log(1 - h(\vec{X}^{(i)}))_k
)
- \frac{\lambda}{2}
\sum_{j=2}^L
\Vert \vec{W}^{(j)} \Vert ^2
)
$$



## 目标函数求导

对于神经网络而言, 无论是线性回归还是逻辑回归, 梯度下降的核心问题依然是损失函数J的求导. 而神经网络的嵌套结构虽然使得其能够表征具有复杂多项式的目标函数, 但若不能有效地优化梯度下降的效率, 那神经网络依然不具备优势. 

而幸好地是, 神经网络的复杂层次嵌套结构使得梯度下降的效率大大提高, **为了简化问题, 我们假设神经网络为二项分布的逻辑回归问题或线性回归问题, 输出层有且仅有一个单元, 且仅有一个样本集**, 假设我们某个权重w求取损失函数的偏微分:

$$
\frac
{\partial J(\vec{W})}
{\partial w}
$$

我们假设一个中间向量Z:

$$
\vec{Z}^{(j)} = 
\vec{W}^{(j)} \cdot \vec{A}^{(j - 1)}\\
z^{(j)} = 
\sum_{i=0}^{S_{(j-1)}}w_{i}^{(j)}a_i^{(j-1)}
$$

由线性回归和逻辑回归的梯度下降求导可以得知, 无论是二项分布的逻辑回归问题或线性回归问题, 他们的偏微分都表示为:

$$
\frac{\partial J(\vec{W})}{\partial w}
= 
(a^{(L)} - y)
\frac{\partial z^{(L)}}{\partial w}
$$

$$
\frac{\partial z^{(L)}}{\partial w} =
\frac{\partial}{\partial w}
\sum_{i=0}^{S_{(L-1)}}w_{i}^{(L)}a_i^{(L-1)}
$$

$$
= 
\sum_{i=1}^{S_{(L-1)}}
w_i^{(L)}
\frac
{\partial a_i^{(L-1)}}{\partial w}
+ 
\frac{\partial(w_0^{(L)}a_0^{(L)})}{\partial w}
$$

$$
= \sum_{i=1}^{S_{(L-1)}}
w_i^{(L)}
\frac{\partial a_i^{(L-1)}}{\partial z_i^{(L-1)}}
\frac{\partial z_i^{(L-1)}}{\partial w}
$$

$$
\frac{\partial z^{(j)}}{\partial w}
=
\sum_{i=1}^{S_{(j-1)}}
w_i^{(j)}
\frac{\partial a_i^{(j-1)}}{\partial z_i^{(j-1)}}
\frac{\partial z_i^{(j-1)}}{\partial w}
$$

上述的这种嵌套结构, 可以通过反向传播算法, 有效地降低损失函数最小化时的计算量.

然而这种嵌套结构还是过于复杂了, 我们假设存在一个中间变量delta:

$$
\delta^{(L)} = a^{(L)} - y\\
\delta^{(L - 1)} = \delta^{(L)}\vec{W}^{(L-1)} 
\cdot 
g'(Z^{(L-1)})\\
\delta^{(j - 1)} = \delta^{(j)}\vec{W}^{(j-1)} 
\cdot 
g'(Z^{(j-1)})\\
$$

因此对于第l层的第i个单元的第j个权重而言, 其偏微分可以简化为:

$$
\frac{\partial J(\vec{W})}{\partial w_{ij}^{(l)}}
= a_j^{(l)} \delta_i^{(l+1)}
$$

从以上的计算过程中我们可以看出, 在对神经网络的损失函数求取偏微分时, 有大量的重复运算可以优化.

## 正向传播

所谓正向传播, 即是从特征向量X开始, 计算每一层逻辑单元的激励值a, 最终通过输出层, 得到预测的y值. 每个单元的的激励值, 都可以通过上一层激励值或特征向量计算得出.

$$
a_i^{(j)} = g(\vec{W}_{i}^{(j)} \cdot \vec{A}^{(j-1)})
$$

因此我们计算出了神经网络中每一个逻辑单元的激励值, 用以进行下一步的反向传播.

## 反向传播

当我们通过正向传播获得了每个逻辑单元的激励值后, 每个单元的delta便可以通过相反方向依次计算得出.

### 逻辑回归

对于激活函数g为sigmoid函数的逻辑回归而言:

$$
\delta^{(L)} = a^{(L)} - y\\
\delta_i^{(L - 1)} = \delta^{(L)}w_i^{(L)}a_i^{(L-1)}(1-a_i^{(L-1)})\\
\delta_i^{(L-2)} = \delta^{(L-1)}w_i^{(L-1)}a_i^{(L-2)}(1-a_i^{(L-2)})\\
$$

直到反向到达了输入层:

$$
\delta_i^{(1)} = \delta^{(2)}w_i^{(2)}a_i^{(1)}(1-a_i^{(1)}) = \delta^{(2)}w_i^{(2)}x_i(1-x_i)
$$

因此我们计算出了神经网络中每一个单元的delta, 用以进行下一步的梯度下降.



