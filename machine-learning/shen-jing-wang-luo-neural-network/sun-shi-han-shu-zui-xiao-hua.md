---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 损失函数最小化

神经网络可以表征线性回归和逻辑回归, 其中主要的差别在于激励函数g的区别. 在这里, 我们使用最常见的sigmoid函数作为逻辑回归的激励函数, 并使用梯度下降进行损失函数最小化.

## 线性回归

当表征线性函数时, 损失函数为:

$$
J(\vec{W}) 
= \frac{1}{2m}
(\sum_{i=1}^m
(
h(\vec{X}^{(i)})
- y^{(i)}
)^2 + 
\lambda
\sum_{j=2}^L
\Vert \vec{W}^{(j)} \Vert ^2
)
$$

$$
\Vert \vec{W}^{(j)} \Vert ^2 = 
\sum_{i=1}^{S_j}
\sum_{j=0}^{S_{(j - 1)}}
w_{ij}^{(j)}
$$

## 逻辑回归

当使用神经网络表征逻辑回归时, 因为逻辑回归的不同分布, 需要将各个可能项的损失函数累加:

$$
J(\vec{W}) =
-\frac{1}{m}(
\sum_{i=1}^m
\sum_{k=1}^K
(
y_k^{(i)} \times log(h(\vec{X}^{(i)}))_k + 
(1 - y_k^{(i)}) \times log(1 - h(\vec{X}^{(i)}))_k
)
- \frac{\lambda}{2}
\sum_{j=2}^L
\Vert \vec{W}^{(j)} \Vert ^2
)
$$



## 正向传播

所谓正向传播, 即是从特征向量X开始, 计算每一层逻辑单元的激励值a, 最终通过输出层, 得到预测的y值. 每个单元的的激励值, 都可以通过上一层激励值或特征向量计算得出.

## 误差传递

让我们先将目光聚焦在输出层和最后一层隐藏层的关系当中:

$$
\vec{A}^{(L)} = g(\vec{W}^{(L)} \cdot \vec{A}^{(L-1)} )
$$

可以看出, 如果我们认为A\(L - 1\)即是特征向量, 那输出层和最后一层隐藏层本质上和一般的回归分析一样.



## 反向传播

神经网络的计算过程非常复杂



