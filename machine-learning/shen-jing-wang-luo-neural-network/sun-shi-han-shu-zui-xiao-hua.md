---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 损失函数最小化

神经网络可以表征线性回归和逻辑回归, 其中主要的差别在于激励函数g的区别. 在这里, 我们使用最常见的sigmoid函数作为逻辑回归的激励函数, 并使用梯度下降进行损失函数最小化.

## 线性回归

当表征线性函数时, 损失函数为:

$$
J(\vec{W}) 
= \frac{1}{2m}
(\sum_{i=1}^m
(
h(\vec{X}^{(i)})
- y^{(i)}
)^2 + 
\lambda
\sum_{j=2}^L
\Vert \vec{W}^{(j)} \Vert ^2
)
$$

$$
\Vert \vec{W}^{(j)} \Vert ^2 = 
\sum_{i=1}^{S_j}
\sum_{j=0}^{S_{(j - 1)}}
w_{ij}^{(j)}
$$

## 逻辑回归

当使用神经网络表征逻辑回归时, 因为逻辑回归的不同分布, 需要将各个可能项的损失函数累加:

$$
J(\vec{W}) =
-\frac{1}{m}(
\sum_{i=1}^m
\sum_{k=1}^K
(
y_k^{(i)} \times log(h(\vec{X}^{(i)}))_k + 
(1 - y_k^{(i)}) \times log(1 - h(\vec{X}^{(i)}))_k
)
- \frac{\lambda}{2}
\sum_{j=2}^L
\Vert \vec{W}^{(j)} \Vert ^2
)
$$



## 梯度下降

对于神经网络而言, 无论是线性回归还是逻辑回归, 梯度下降的核心问题依然是损失函数J的求导. 而神经网络的嵌套结构虽然使得其能够表征具有复杂多项式的目标函数, 但若不能有效地优化梯度下降的效率, 那神经网络依然不具备优势. 

而幸好地是, 神经网络的复杂层次嵌套结构使得梯度下降的效率大大提高, 为了简化问题, 我们假设神经网络为二项分布的逻辑回归问题或线性回归问题, 输出层有且仅有一个单元, 且仅有一个样本集, 假设我们对第l层第j个单元的第i个权重求取损失函数的偏微分:

$$
\frac
{\partial J(\vec{W})}
{\partial w_{ji}^{(l)}}
$$

由线性回归和逻辑回归的梯度下降求导可以得知, 无论是二项分布的逻辑回归问题或线性回归问题, 他们的偏微分都表示为:

$$
\frac{\partial J(\vec{W})}{\partial w_{ji}^{(l)}}
= 
(h(\vec{A}^{(L)}) - y)
\frac{\partial (\vec{W}^{L} \cdot \vec{A}^{(L - 1)})}{\partial w_{ji}^{(l)}}
$$

## 正向传播

所谓正向传播, 即是从特征向量X开始, 计算每一层逻辑单元的激励值a, 最终通过输出层, 得到预测的y值. 每个单元的的激励值, 都可以通过上一层激励值或特征向量计算得出.

$$
a_i^{(j)} = g(\vec{W}_{i}^{(j)} \cdot \vec{A}^{(j-1)})
$$

## 误差传递

让我们先将目光聚焦在输出层和最后一层隐藏层的关系当中:

$$
a_i^{(L)} = g(\vec{W}_{i}^{(L)} \cdot \vec{A}^{(L-1)})
$$

可以看出, 如果我们认为A\(L - 1\)即是特征向量X, 那输出层和最后一层隐藏层本质上和一般的回归分析一样. 因此可以通过公式计算得出输出层的误差:

$$
\delta y = 
\delta a^{(L)} \\
= h(\vec{X}) - y\\
= g(\vec{W}^{(L)} \cdot \vec{A}^{(L-1)} ) - y
$$

这种误差的产生, 我们猜测

## 反向传播

神经网络的计算过程非常复杂



