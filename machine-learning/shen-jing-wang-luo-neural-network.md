---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 神经网络\(Neural Network\)

## 什么是神经网络

神经网络是一种尝试模仿人类大脑神经系统的算法, 它逐渐兴起于二十世纪八九十年代并受到广泛应用, 后续因为其偏大的计算量而沉寂过一段时间, 如今随着计算机运算能力的成长, 它再一次进入到我们的视野并且成为了一个十分有用的工具.

人类的大脑十分神奇, 它可以学习各种新的知识和技能. 和别的器官不一样, 人体的其他器官都存在具体的用途, 因此都在结构上进行了分化, 例如眼球能够感知光线, 肺部能够吸收氧气, 不过我们并不能通过眼球来呼吸, 也不能通过肺部来观察世界, 但是大脑可以!

科学家们在近代发现, 大脑不同部位会负责不同的工作, 这似乎意味着, "视觉"的脑区并不能替代"触觉"脑区, 我们的大脑里装着各种各样不同功能的脑区, 就像电脑里面运行着各种不同功能的程序一样. 

随后科学家们通过神经重接试验, 将视觉与触觉的感受器神经与脑区交换连接, 最终视觉脑区学会了"触觉", 触觉脑区学会了"视觉".

我们可以推断, 我们的大脑存在一种非常"高明"的认知机制, 这种认知机制能够模仿其他任何认知.我们推测, 这种认知机制是依靠无数个结构相对简单的神经元细胞间复杂的连接来实现的.

## 神经元

神经元是神经网络中的基础单位, 他主要包含了三个功能结构:

* 树突 - 接受神经电信号
* 细胞体 - 处理神经电信号
* 轴突 - 向另一个神经元传递神经电信号

![&#x6765;&#x6E90;&#x4E8E;&#x7F51;&#x7EDC; http://tupian.baike.com/](../.gitbook/assets/20300000356220132378308333906%20%281%29.jpeg)

这种神经元结构非常类似于目标函数h, 同样可以接受自变量集合X, 经过算法处理后, 输出一个结果值y. 

## 逻辑单元\(Logic Unit\)

在神经网络中, 我们模仿神经元构建出了逻辑单元, 我们用字母U来表示逻辑单元, 每个逻辑单元同样都包含了三个部分:

* 自变量集合X - 类似于神经元的树突, 用于接收上一层逻辑单元的因变量y
* 激励函数s - 类似于神经元的细胞体, 用于处理自变量集合X并输出激励函数的因变量y
* 因变量y - 类似于神经元的轴突, 将作为下一层逻辑单元的自变量集合X的元素

![&#x6765;&#x6E90;&#x4E8E;&#x7F51;&#x7EDC; https://blog.csdn.net/u012328159/article/details/51143536](../.gitbook/assets/mp.gif)

自变量集合X在传入激励函数s之前, 会经过一定的系数转换, 这些系数w被称为自变量x的**权重\(Weight\)**. 它和目标函数h的系数θ本质上是一样的, 都是函数的系数, 但是权重这个术语主要用于神经网络, 且他们在使用范围上存在区别. 

逻辑单元和神经元一样, 都是各自网络系统中的基本单位. 因此神经网络中存在多个逻辑单元, 每个逻辑单元都有自己的权重集合W和自变量集合X, 通过激励函数s计算出因变量y.

而神经网络, 也即是多个层次, 多个逻辑单元共同组合成了我们的目标函数h, 不能单纯地认为权重集合W等于目标函数h的系数集合Θ.

## 神经网络层次结构

神经网络仅仅是一个逻辑单元是不够的, 毕竟一个逻辑单元能做的事情, 线性回归和逻辑回归也能做到, 他们之间并没有什么本质的区别. 那神经网络的优势究竟在哪里?

神经网络真正的优势是在于其"无限可能"的层次结构. 多个逻辑单元互相连接互相影响. 就如同人类大脑一样, 神经网络的层次结构能够模拟和实现任何目标函数, 同时还能够兼顾性能, 这才是神经网络的最大优势.

![&#x6765;&#x6E90;&#x4E8E;&#x7F51;&#x7EDC; https://blog.csdn.net/u012328159/article/details/51143536](../.gitbook/assets/20160413212059719.jpg)

上图是一个层次非常简单的神经网络, 每个圆圈都是一个单元, 并且包含了三层结构, 其中`LLayer1   ayer1Layer1`是输入层, `Layer2`是隐藏层, `Layer3`是输出层. 每一层结构, 都含有一个特殊的单元, 被称为**偏置单元\(Bias Unit\)**. 他们的值都恒定为1, 为下一层的输入提供了常量.

![&#x6765;&#x6E90;&#x4E8E;&#x7F51;&#x7EDC; https://blog.csdn.net/lsh\_2013/article/details/47454079](../.gitbook/assets/th-1.jpeg)

### 输入层

即特征向量X, 输入层固定有n+1个单元, x1, x2, ... , xn代表了n个特征值. x0为偏置单元, 代表了常量, 其值恒定为1. 每个神经网络有且仅有一个输入层.

### 隐藏层

每个神经网络

### 输出层

### 层次嵌套

从神经网络多层次的结构我们可以看出, 神经网络事实上是一种嵌套结构, 一个层次深的神经网络可以拆分成多个小的神经网络, 任何一个输出层都可以视为另一个神经网络输入层的一个单元, 任何一个输入层都可以视为多个神经网络组合的输出层.

## 神经网络能做什么

神经网络算法是通过模仿神经元以及神经元间复杂的连接来实现的. 不过遗憾地是, 因为计算机运算能力的制约, 神经网络算法目前并不能真正地模仿大脑, 但是它为我们很好地解决了高维度特征向量的多项组合过多的问题.

在现实中, 我们面对的往往是高纬度的特征向量X, 且特征值之间可能存在交叉影响, 假设我们对一个特征向量维度为10的问题进行线性回归, 并且目标函数的次数为1, 则:

$$
h(X) = \theta_0 + \theta_1x_1 + ... + \theta_{10}x_{10}\\
+ \theta_{11}x_1x_2 + \theta_{12}x_1x_3 +  \theta_{13}x_1x_4\\
+ ... + \theta_{1024}x_1x_2x_3x_4x_5x_6x_7x_8x_9x_{10}
$$

仅仅特征向量维度n=10的线性回归问题, 当特征值之间存在交叉影响, 参数集合Θ的大小都会呈现几何级数增长, 对于任意n元a次函数而言, 参数集合Θ的大小为:

$$
(a+1)^n
$$

这种数量级, 使用单纯的梯度下降来求解损失函数最小化, 已经不是这个时代的计算机能够完成的任务了!

## 一些约定

在神经网络范畴内, 有一些术语区别于线性回归和逻辑回归, 我们需要对这些术语进行约定:

### 单元

每个神经网络都喊有多个单元, 我们使用字母U来表示单元

### 层数

神经网络一般至少含有三个层次, 我们约定输入层的层数为1, 使用字母j来表示

### 序数

### 权重

神经网络中, 每个单元都有各自的权重集合W, 

### 激励值

激励值即自变量集合X通过激励函数s计算后得到的数值, 和因变量y在意义上是等价的. 那为什么还要多余定义一个激励值呢? 这更多地是为了表述的方便, 因为因变量y的含义在很多范畴上都存在. 为了仅针对神经网络隐藏层中激励函数s计算出的过程值, 我们定义了激励值, 并用a来表述:

$$
a_i^j = s(w)
$$



