---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 线性回归 \(Line Regression\)

## 什么是线性回归

所谓的回归, 即我们预测同一组数据中, 不同的变量之间存在某种可以被定量的关系. 为了能够得到这种关系的函数表达形式, 我们利用统计学上的观点, 认为在数据样本足够多的情况下, 可以根据样本反推出这种关系. 之所以是分析, 则是因为这种反推存在不确定性, 需要我们多次提出假设后进行校验. 而线性回归是最常见的一种回归分析. 在现实意义的角度来看, 其最重要的特征就是预测的结果是**连续**. 例如:

* 分析猪肉价格和通货膨胀的关系, 为线性回归
* 分析父母的身高与子女身高, 为线性回归
* 分析肿瘤大小与是否为恶性肿瘤的关系, 为非线性回归

可以看出, 无论是通货膨胀还是子女身高, 都是一个连续区间的取值, 而是否为恶性肿瘤则只是二项分布. 所以线性回归的结果必然是连续的.

## 损失函数 \(Cost Function\)

当我们在进行机器学习的时候, 我们的目标是在函数集H内找到一个函数h, 这个函数h能够很好的拟合当前的训练集D和一些未知的测试数据. 而要找出这个函数h, 我们需要一些依据或者标准, 那就是**损失函数**, 不需要对这个名词过于恐慌 \(这些专有名词总是好像哈利波特的咒语一样故弄玄虚\). 他在本质上只是函数, 不过它用于衡量另外一个函数是否令人满意, 仅此而已.

我们使用大写字母J代表损失函数, 根据不同的理论和角度, 损失函数有很多种类型, 而不同类型的损失函数在不同的使用场景中效果也不尽相同. 在这里我们使用最常见的**方差\(Squre Loss\)**作为我们的损失函数:

$$
J(\Theta) = \frac{1}{2m}\sum_{i=1}^m(h(X^{(i)}) - y^{(i)})^2
$$

其中:

$$
\frac{1}{m}\sum_{i=1}^m(h(X^{(i)}) - y^{(i)})^2
$$

便是目标函数h与样本输出值y的方差和和. 

为什么损失函数要在方差的基础上除以2呢? 原因是为了在未来进行梯度下降寻找最小的损失函数值时, 能够将方差微分后的2次方项抵消. 



