---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 一些约定

#### m

 代表样本的数量

#### x

 代表输入值, 自变量或特征值

#### n

代表了每个样本中输入值的数量, 即样本中特征向量的维度

#### X

代表了样本的特征向量或输入值的集合, 一般以以下形式表示:

$$
X = \{x_1, x_2, x_3, ..., x_n\}
$$

#### y

 代表输出值, 因变量或结果值

#### \(X, y\)

 代表了一个样本, 以下代表了第i个样本:

$$
(X^{(i)}, y^{(i)})
$$

#### h \(hypothesis\)

 代表了机器学习训练后需要输出的目标函数\(假设\), 它以X为自变量, y为因变量, 通常以一下数学形式表示:

$$
h = X \Rightarrow y
$$

#### θ \(theta\)

代表了目标函数内输入值x的参数, 通常有 n + 1 个, 与X一起构建了目标函数h \(假设为n元一次函数\):

$$
h(X) = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n
$$

为了使X与θ能够对称, 我们假设存在一个永远等于1的自变量x0, 表达式子如下:

$$
h(X) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n
$$

当目标函数代入了样本后, 其表达式如下:

$$
h(X^{(i)}) = \theta_0x_0^{(i)} + \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + ... + \theta_nx_n^{(i)}
$$

#### J

代表了目标函数h的损失函数, 注意其自变量是目标函数的参数θ:

$$
J(\theta_0, \theta_1, \theta_2, ..., \theta_n) = \frac{1}{2m}\sum_{i=1}^m(h(X^{(i)}) - y^{(i)})^2
$$



