---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 一些约定

## m

 代表样本的数量

## x

 代表输入值, 自变量或特征值

## n

代表了每个样本中输入值的数量, 即样本中特征向量的维度

## X

代表了样本的特征向量或输入值的集合, 一般以以下形式表示:

$$
X = \{x_1, x_2, x_3, ..., x_n\}
$$

## y

 代表输出值, 因变量或结果值

## \(X, y\)

 代表了一个样本, 以下代表了第i个样本:

$$
(X^{(i)}, y^{(i)})
$$

## D

代表了训练集, 也即是所谓的特征项集合:

$$
D = \{ (X^{(i)}, y^{(i)}) | 1 \leq i \leq m \}
$$

当我们将训练集D视为一个矩阵时, 矩阵中的元素可以通过下标表示:

$$
D = \left [
\begin{matrix}
1 & 2 & 3\\
4 & 5 & 6\\
7 & 8 & 9
\end{matrix}
\right ]\\
D_{11} = 1, D_{21} = 4
$$

## h \(hypothesis\)

代表了机器学习训练后需要输出的目标函数\(假设\), 它以X为自变量, y为因变量, 通常以以下数学形式表示:

$$
h = X \Rightarrow y
$$

## θ \(theta\)

代表了目标函数内输入值x的参数, 一般至少有 n + 1 个, 与X一起构建了目标函数h \(假设为n元一次函数, 且不存在交叉影响\):

$$
h(X) = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n
$$

为了使X与θ能够对称, 我们假设存在一个永远等于1的自变量x0, 表达式子如下:

$$
h(X) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n
$$

当目标函数代入了样本后, 其表达式如下:

$$
h(X^{(i)}) = \theta_0x_0^{(i)} + \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + ... + \theta_nx_n^{(i)}
$$

## Θ \(Theta\)

这是θ的大写形式, 代表了目标函数内参数的集合:

$$
\Theta = \{ \theta_0, \theta_1, \theta_2, ..., \theta_n  \}
$$

## H \(Hypothesis set\)

代表了所有可能的目标函数的集合, 在实践中, 我们通过经验或理论, 总是能对目标函数h提出一种或多种假设, 例如假设目标函数为n元n次函数, 则会得到函数集H1:

$$
H_1 = \{h | h(X) = \theta_0 + \theta_1x_1 + \theta_2x_1^2 + ... + \theta_nx_n^n \}
$$

或者我们假设目标函数为n元二次函数, 则会得到函数集H2:

$$
H_2 = \{h | h(X) = \theta_0 + \theta_1x_1^2 + \theta_2x_1^2 + ... + \theta_nx_n^2 \}
$$

## J

代表了目标函数h的损失函数, 注意其自变量是目标函数的参数θ:

$$
J(\Theta) = \frac{1}{m}\sum_{i=1}^mCost(h_\theta(X^{(i)}), y^{(i)})
$$

其中, 线性回归和逻辑回归的损失函数在Cost部分并不相同

