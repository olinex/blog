---
description: '本文章由olinex原创, 转载请在页面开头标明出处'
---

# 维数约减\(Dimensionality Reduction\)

## 主成分分析法\(Principal Component Analysis\)

为了能够有效地进行维数约减, 我们要避免在降维的时候数据"失真", 因为在进行维数约减的同时, 会人为地引入误差. 

### 归一化处理

在进行主成分分析之前, 我们需要对数据进行归一化处理. 这是非常重要的一步, 因为我们在对比不同维度的特征值的"重要程度"的时候, 不希望被量纲和取值范围的不同, 影响了我们的判断. 因为当一项特征值的取值范围远大于另一项的特征值时, 前者微小的变化可能都会比后者的最大值都要大. 这会掩盖掉后者重要性.

### 降维

当我们需要将样本集的数据从n维降为k维时, 可以通过两个步骤进行. 

首先要算出样本集的**协方差矩阵\(Covariance Matrix\)**, 他是一个n x n的矩阵:

$$
\epsilon
=
\frac{1}{m}
\sum_{i=1}^{n}(X^{(i)})(X^{(i)})^T
$$

$$
(X^{(i)})(X^{(i)})^T
=
\left [
\begin{matrix}
x_{1}^{(i)}x_{1}^{(i)} & 
x_{1}^{(i)}x_{2}^{(i)} & 
... & 
x_{1}^{(i)}x_{n}^{(i)}\\
x_{2}^{(i)}x_{1}^{(i)} & 
x_{2}^{(i)}x_{2}^{(i)} & 
... & 
x_{2}^{(i)}x_{n}^{(i)}\\
...\\
x_{n}^{(i)}x_{1}^{(i)} & 
x_{n}^{(i)}x_{2}^{(i)} & 
... & 
x_{n}^{(i)}x_{n}^{(i)}\\
\end{matrix}
\right ]\\
$$

接着我们通过协方差矩阵, 可以通过特征值分解计算出协方差矩阵的**特征向量\(Eigenvectors\)**, 以Octive语言为例:

```text
[U, S, V] = svd(Sigma)
```

其中, U也为n x n的矩阵, 截取它的前K列元素, 即为n x k的矩阵, 我们将这个矩阵命名为U\(reduce\). 最终要获得k维的样本, 通过以下公式可以获得:

$$
Z_{k \times 1}^{(i)} = (U_{(reduce)})^TX^{(i)}
$$

